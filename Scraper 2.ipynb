{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#make sure plots are embedded into the notebook\n",
    "%pylab inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Starting 1 threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Configuration variable 'use_api_login' is defined but unknown.\n",
      "Misspelled?\n",
      "WARNING: Running on Windows and transliteration_target is not set.\n",
      "Please see https://www.mediawiki.org/wiki/Special:MyLanguage/Manual:Pywikibot/Windows\n",
      "WARNING: family and mylang are not set.\n",
      "Defaulting to family='test' and mylang='test'.\n"
     ]
    }
   ],
   "source": [
    "import pywikibot as pwb\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "# Make plots prettyful\n",
    "pl.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = [\n",
    "    ('en', 'English'), \n",
    "    ('de', 'German'),\n",
    "    ('es', 'Spanish'),\n",
    "    ('fr', 'French'),\n",
    "    ('ja', 'Japanese'),\n",
    "    ('ru', 'Russian'),\n",
    "    ('it', 'Italian'),\n",
    "    ('pt', 'Portuguese'),\n",
    "    ('zh', 'Chinese'),\n",
    "    ('nl', 'Dutch'),\n",
    "    ('pl', 'Polish'),\n",
    "    ('ar', 'Arabic'),\n",
    "    ('tr', 'Turkish'),\n",
    "    ('sv', 'Swedish'),\n",
    "    ('ko', 'Korean'),\n",
    "    ('fa', 'Persian'),\n",
    "    ('uk', 'Ukrainian'),\n",
    "    ('he', 'Hebrew'),\n",
    "    ('id', 'Indonesian'),\n",
    "    ('cs', 'Czech'),\n",
    "    ('ur', 'Urdu'),\n",
    "    ('hi', 'Hindi')\n",
    "]\n",
    "\n",
    "lang_codes = [i[0] for i in langs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = pd.read_csv('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ar', u'cs', u'de', u'en', u'es', u'fa', u'fr', u'he', u'hi', u'id',\n",
       "       u'it', u'ja', u'ko', u'nl', u'pl', u'pt', u'ru', u'sv', u'tr', u'uk',\n",
       "       u'ur', u'zh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset100 = cities[cities['en'].isin([\n",
    "    u'Shanghai',\n",
    "    u'Karachi',\n",
    "    u'Lagos',\n",
    "    u'Istanbul',\n",
    "    u'Tokyo',\n",
    "    u'Mumbai',\n",
    "    u'Moscow',\n",
    "    u'São Paulo',\n",
    "    u'Seoul',\n",
    "    u'New York City'\n",
    "])][['en','zh','tr','ja','ru','pt','ko','ur','hi','de']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset25 = cities[cities['en'].isin([\n",
    "    u'Shanghai',\n",
    "    u'Istanbul',\n",
    "    u'Tokyo',\n",
    "    u'Moscow',\n",
    "    u'New York City'\n",
    "])][['en','zh','tr','ja','ru']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "      <th>tr</th>\n",
       "      <th>ja</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>上海市</td>\n",
       "      <td>Şanghay</td>\n",
       "      <td>上海市</td>\n",
       "      <td>Шанхай</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Istanbul</td>\n",
       "      <td>伊斯坦堡</td>\n",
       "      <td>İstanbul</td>\n",
       "      <td>イスタンブル</td>\n",
       "      <td>Стамбул</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>東京都</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>東京都</td>\n",
       "      <td>Токио</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Moscow</td>\n",
       "      <td>莫斯科</td>\n",
       "      <td>Moskova</td>\n",
       "      <td>モスクワ</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New York City</td>\n",
       "      <td>纽约</td>\n",
       "      <td>New York</td>\n",
       "      <td>ニューヨーク</td>\n",
       "      <td>Нью-Йорк</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               en    zh        tr      ja        ru\n",
       "0        Shanghai   上海市   Şanghay     上海市    Шанхай\n",
       "4        Istanbul  伊斯坦堡  İstanbul  イスタンブル   Стамбул\n",
       "5           Tokyo   東京都     Tokyo     東京都     Токио\n",
       "9          Moscow   莫斯科   Moskova    モスクワ    Москва\n",
       "20  New York City    纽约  New York  ニューヨーク  Нью-Йорк"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Queue to process pages\n",
    "# Keep tuples of (title,depth)\n",
    "# Where depth is the current depth, don't go past depth\n",
    "queue = []\n",
    "queued = {}\n",
    "\n",
    "def scrapeCity(graph,lang,city,cities,max_depth=3):\n",
    "    if max_depth < 1:\n",
    "        return\n",
    "    \n",
    "    site = pwb.Site(lang, 'wikipedia')\n",
    "    \n",
    "    queue.append((pwb.Page(site, city),0))\n",
    "    queued[city] = None\n",
    "    \n",
    "    while queue:\n",
    "        page, depth = queue.pop()\n",
    "        title = page.title()\n",
    "        \n",
    "        #if depth < max_depth:\n",
    "        for p in page.linkedPages(namespaces=0):\n",
    "            if p.namespace():\n",
    "                continue\n",
    "                \n",
    "            t = p.title()\n",
    "\n",
    "            graph.add_edge(title,t)\n",
    "\n",
    "            # If we haven't queued/processed the page yet\n",
    "            # And it isn't one of the cities.\n",
    "            if t not in queued and not (t == cities).any():\n",
    "                if depth < (max_depth - 1):\n",
    "                    queued[t] = None\n",
    "                    queue.append((pwb.Page(site,t), depth+1))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: c:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pandas\\core\\ops.py:562: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  result = lib.scalar_compare(x, y, op)\n",
      "\n",
      "WARNING:py.warnings:c:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pandas\\core\\ops.py:562: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  result = lib.scalar_compare(x, y, op)\n",
      "\n",
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n",
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n",
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh 上海市\n",
      "zh"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 伊斯坦堡\n",
      "zh"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 東京都\n",
      "zh 莫斯科\n",
      "zh"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 纽约\n",
      "tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:tr processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Şanghay\n",
      "tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:tr processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " İstanbul\n",
      "tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:tr processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokyo\n",
      "tr Moskova\n",
      "tr New York\n",
      "ja"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ja processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 上海市\n",
      "ja"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ja processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " イスタンブル\n",
      "ja"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ja processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 東京都\n",
      "ja"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ja processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " モスクワ\n",
      "ja"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ja processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ニューヨーク\n",
      "ru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ru processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Шанхай\n",
      "ru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ru processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Стамбул\n",
      "ru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ru processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Токио\n",
      "ru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ru processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Москва\n",
      "ru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:ru processes running, including this one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Нью-Йорк\n"
     ]
    }
   ],
   "source": [
    "for lang in subset20.columns:\n",
    "    queue = []\n",
    "    queued = {}\n",
    "    df = nx.DiGraph()\n",
    "    \n",
    "    for city in subset25[lang]:\n",
    "        print lang,city\n",
    "        scrapeCity(df, lang, city.decode('utf-8'), subset25[lang], max_depth=2)\n",
    "        \n",
    "    nx.write_gpickle(df,'%s25.pickle' % lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset20 = subset25[['zh','tr','ja','ru']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>tr</th>\n",
       "      <th>ja</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>上海市</td>\n",
       "      <td>Şanghay</td>\n",
       "      <td>上海市</td>\n",
       "      <td>Шанхай</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>伊斯坦堡</td>\n",
       "      <td>İstanbul</td>\n",
       "      <td>イスタンブル</td>\n",
       "      <td>Стамбул</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>東京都</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>東京都</td>\n",
       "      <td>Токио</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>莫斯科</td>\n",
       "      <td>Moskova</td>\n",
       "      <td>モスクワ</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>纽约</td>\n",
       "      <td>New York</td>\n",
       "      <td>ニューヨーク</td>\n",
       "      <td>Нью-Йорк</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zh        tr      ja        ru\n",
       "0    上海市   Şanghay     上海市    Шанхай\n",
       "4   伊斯坦堡  İstanbul  イスタンブル   Стамбул\n",
       "5    東京都     Tokyo     東京都     Токио\n",
       "9    莫斯科   Moskova    モスクワ    Москва\n",
       "20    纽约  New York  ニューヨーク  Нью-Йорк"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = pwb.Site('zh', 'wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-84a1931e0e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpwb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset20\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.decode('utf-8'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pywikibot\\tools\\__init__.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                              cls, depth)\n\u001b[0;32m   1104\u001b[0m                     \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pywikibot\\tools\\__init__.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*__args, **__kw)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                              cls, depth)\n\u001b[0;32m   1104\u001b[0m                     \u001b[1;32mdel\u001b[0m \u001b[0m__kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_arg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pywikibot\\page.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, title, ns)\u001b[0m\n\u001b[0;32m   1937\u001b[0m                 raise ValueError(u'Title must be specified and not empty '\n\u001b[0;32m   1938\u001b[0m                                  'if source is a Site.')\n\u001b[1;32m-> 1939\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecate_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_redirect\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pywikibot\\page.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, title, ns)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpywikibot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseSite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_link\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaultNamespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_revisions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jeremy\\Anaconda\\lib\\site-packages\\pywikibot\\page.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, source, defaultNamespace)\u001b[0m\n\u001b[0;32m   4479\u001b[0m         \u001b[1;31m# preprocess text (these changes aren't site-dependent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4480\u001b[0m         \u001b[1;31m# First remove anchor, which is stored unchanged, if there is one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4481\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;34mu\"|\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4482\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_anchor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4483\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "page = pwb.Page(site, subset20['zh'][0])#.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE:pywiki:Found 1 wikipedia:zh processes running, including this one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Page(上海市)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subset20['zh'][0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лихтенштейн на летних Олимпийских играх 1948\n"
     ]
    }
   ],
   "source": [
    "print df.nodes()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ja = nx.read_gpickle('ru25.pickle')\n",
    "nx.write_graphml(ja, 'ru25.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
